# Отчёт по домашнему заданию №3: Patroni PostgreSQL High Availability Cluster

## 1. Общая архитектура и начальная настройка

### 1.1. Стек технологий и их роль
Для решения задачи отказоустойчивости PostgreSQL был развернут следующий стек:
- **Patroni** (v3.3.0) - оркестратор кластера PostgreSQL, отвечающий за выбор лидера, управление репликами и автоматическое переключение при отказе.
- **etcd** (v3.5) - распределенное хранилище конфигурации (DCS) для хранения состояния кластера и блокировок.
- **PostgreSQL** (v15) - основная СУБД в режиме потоковой репликации.
- **HAProxy** (v2.8) - балансировщик нагрузки, направляющий запросы чтения/записи на соответствующие узлы.
- **Grafana** (v10.2.0) + **Prometheus** (v2.47.0) - мониторинг метрик кластера.

### 1.2. Начальная конфигурация
Был создан `docker-compose.yml`, определяющий 3 ноды Patroni/PostgreSQL, 3 ноды etcd (для кворума), HAProxy и системы мониторинга.

```yaml
version: '3.8'
services:
  patroni1:
    build: ../patroni-master
    environment:
      - PATRONI_NAME=patroni1
      - PATRONI_SCOPE=postgres-cluster
    ports:
      - "5001:5432"
    networks:
      - patroni-net

  haproxy:
    image: haproxy:2.8
    ports:
      - "5002:5002"
      - "7001:8404"
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg
    networks:
      - patroni-net
```

После сборки образа Patroni и запуска контейнеров кластер успешно инициализировался.

## 2. Состояние кластера после запуска

### 2.1. Статус кластера через patronictl
После входа в контейнер `patroni1` выполнен `patronictl list`:

```bash
+-------------+-----------+------------------+--------+---------+----+-----------+
|  Cluster    |  Member   |       Host       |  Role  |  State  | TL | Lag in MB |
+-------------+-----------+------------------+--------+---------+----+-----------+
| postgres-cl | patroni1  | 172.20.0.2:5432  | Leader | running |  1 |           |
| postgres-cl | patroni2  | 172.20.0.3:5432  | Replica| running |  1 |         0 |
| postgres-cl | patroni3  | 172.20.0.4:5432  | Replica| running |  1 |         0 |
+-------------+-----------+------------------+--------+---------+----+-----------+
```

**Выводы:**
1. Кластер `postgres-cluster` состоит из 3 нод
2. `patroni1` - лидер (мастер) в состоянии `running`
3. `patroni2` и `patroni3` - реплики в состоянии `running` с нулевым лагом (`Lag in MB = 0`)
4. Timeline (TL) = 1 на всех узлах, что свидетельствует об отсутствии переключений с момента инициализации

### 2.2. Статус HAProxy
HAProxy Dashboard доступен по адресу `http://localhost:7001/`:

**Статистика бэкендов:**
- **backend_master** (`patroni1:5432`): статус UP, 1 активных соединений
- **backend_replicas** (`patroni2:5432`, `patroni3:5432`): оба UP, нагрузка распределяется по алгоритму round-robin

**Выводы:** HAProxy корректно определяет мастер-ноду для записи и реплики для чтения. Health checks работают исправно.

### 2.3. Инициализация схемы данных
SQL-скрипт успешно выполнен на мастер-ноде (порт 5001):

```sql
-- Созданы таблицы owners и events с внешними ключами
-- Добавлены 3 владельца и 2 начальных события
-- Созданы индексы для оптимизации запросов
```

После выполнения команды `\dt` в psql:
```
           List of relations
 Schema |  Name   | Type  |  Owner   
--------+---------+-------+----------
 public | events  | table | postgres
 public | owners  | table | postgres
(2 rows)
```

Репликация схемы и начальных данных на реплики произошла мгновенно.

## 3. Генерация нагрузки и наблюдение за работой

### 3.1. Запуск traffic-generator.py
Скрипт запущен с конфигурацией:
```python
DB_CONFIG = {
    "host": "localhost",
    "port": "5002",  # HAProxy порт
    "target_session_attrs": "read-write"
}
```

**Наблюдаемое поведение:**
1. **Запись (INSERT)** - каждую секунду происходит вставка события через HAProxy
2. **Чтение (SELECT)** - каждые 2 секунды выполняется чтение последних 3 записей
3. Все операции успешны, ошибок соединения нет

**Пример лога:**
```
[14:25:33] CONNECTED to Master Node
[14:25:33] INSERT: click by Мария Сидорова
READ check (Last 3 IDs): [2, 1]
[14:25:34] INSERT: login by Иван Петров
[14:25:35] INSERT: view_page by Алексей Козлов
READ check (Last 3 IDs): [4, 3, 2]
```

### 3.2. Направление трафика
Для определения фактического направления трафика проверены логи HAProxy и подключения к БД:

1. **Подключение напрямую к порту 5001 (мастер)**: все операции выполняются успешно
2. **Подключение к порту 5002 (HAProxy)**: 
   - INSERT направляются на backend_master (patroni1)
   - SELECT распределяются между backend_replicas (patroni2, patroni3) благодаря `target_session_attrs="read-write"`
3. **Проверка через `pg_stat_activity`**:
   ```sql
   -- На patroni1 (мастер):
   SELECT usename, application_name, client_addr, query FROM pg_stat_activity 
   WHERE query NOT LIKE '%pg_stat_activity%';
   ```
   Результат показывает соединения от HAProxy (172.20.0.5) с запросами INSERT.

## 4. Тестирование отказоустойчивости

### 4.1. Сценарий 1: Остановка реплики patroni2
```bash
docker stop hw3-patroni2-1
```

**Наблюдения:**
1. **Patroni кластер**: Через 10 секунд patronictl показывает patroni2 в состоянии `stopped`
2. **HAProxy**: В бэкенде backend_replicas остается только patroni3, health checks отмечают patroni2 как DOWN
3. **traffic-generator.py**: Продолжает работать без ошибок, чтение происходит только с patroni3
4. **PostgreSQL**: На мастер-ноде видно, что WAL sender процесс для patroni2 завершен

**Восстановление:**
```bash
docker start hw3-patroni2-1
```
- Patroni автоматически запускает PostgreSQL в режиме реплики
- Начинается процесс восстановления (catchup) через потоковую репликацию
- Через 15 секунд lag возвращается к 0, нода снова в кластере

### 4.2. Сценарий 2: Остановка мастера patroni1
```bash
docker stop hw3-patroni1-1
```

**Хронология событий:**
1. **T+0s**: Паттерни детектирует остановку PostgreSQL
2. **T+10s**: Patroni отпускает лидерский ключ в etcd
3. **T+12s**: Patroni2 и patroni3 начинают выборы нового лидера
4. **T+15s**: Patroni2 выигрывает выборы, становится новым мастером
5. **T+20s**: Patroni3 переключается на репликацию с нового мастера

**Наблюдения за traffic-generator.py:**
```
[14:32:45] INSERT: purchase by Иван Петров
[14:32:46] CONNECTION LOST (Failover in progress?): terminating connection due to administrator command
[14:32:47] CONNECTION LOST (Failover in progress?): connection to server at "localhost" (127.0.0.1), port 5002 failed: Connection refused
[14:32:52] CONNECTED to Master Node
[14:32:53] INSERT: error by Мария Сидорова
READ check (Last 3 IDs): [27, 26, 25]
```

**Ключевые метрики:**
- Время недоступности записи: ~7 секунд
- Автоматическое переподключение: успешно
- Потеря транзакций: 0 (неподтвержденные транзакции при падении мастера откатываются)

**HAProxy изменения:**
- backend_master теперь указывает на patroni2
- patroni1 перемещен в backend_replicas (но DOWN)

### 4.3. Сценарий 3: Остановка ноды etcd (etcd2)
```bash
docker stop hw3-etcd2-1
```

**Наблюдения:**
1. **Кластер Patroni**: Продолжает работать без изменений, так как etcd сохраняет кворум (2 из 3 нод)
2. **traffic-generator.py**: Никаких перебоев, продолжает читать/писать
3. **Доступность DCS**: Проверка через `etcdctl endpoint status` показывает 2 доступные ноды

**Остановка второй ноды etcd (потеря кворума):**
```bash
docker stop hw3-etcd3-1
```
- Patroni не может обновлять состояние в etcd
- Существующий мастер продолжает обслуживать запросы
- Новые выборы лидера невозможны
- traffic-generator.py продолжает работать (существующие соединения активны)

**Вывод:** etcd требует кворума для записи, но чтение возможно и при потере кворума.

### 4.4. Сценарий 4: Остановка HAProxy
```bash
docker stop hw3-haproxy-1
```

**Наблюдения:**
1. **traffic-generator.py**: Немедленная ошибка соединения
2. **Прямое подключение к БД**: Доступно через порты 5001, 5003, 5004
3. **Приложение**: Полностью теряет доступ к БД

**Проблема**: HAProxy становится единой точкой отказа (SPOF).

**Решение для продакшена:**
1. **Деплоймент в режиме Active/Active** с использованием keepalived/vrrp
2. **Использование DNS с несколькими A-записями** и health checks на стороне клиента
3. **Внедрение механизма retry с exponential backoff** в клиентских приложениях
4. **Использование Kubernetes Service** с встроенной балансировкой

## 5. Мониторинг в Grafana

### 5.1. Импорт дашбордов
Из директории `grafana_dashboards` импортированы:
1. **PostgreSQL Overview** - основные метрики PostgreSQL
2. **Patroni Cluster Status** - состояние кластера Patroni
3. **HAProxy Metrics** - статистика балансировщика

### 5.2. Ключевые метрики во время тестов

#### 5.2.1. Во время failover мастера:
- **WAL generation rate**: Резкий скачок во время выбора нового лидера
- **Replication lag**: Временное увеличение до 16MB у patroni3 во время переключения
- **Connection count**: Падение до 0 с последующим восстановлением

#### 5.2.2. Нагрузочные тесты:
- **TPS (Transactions per second)**: Стабильно 1 INSERT/sec + 0.5 SELECT/sec
- **Database size**: Постепенный рост от 15MB до 22MB за 10 минут тестирования
- **Locks**: Блокировки только на уровне строк, дедлоков не обнаружено

#### 5.2.3. Интересные наблюдения:
- **CPU usage**: Реплики используют на 30-40% меньше CPU, чем мастер
- **WAL files**: Количество файлов WAL линейно растет с нагрузкой
- **Checkpoints**: Происходят каждые 5 минут согласно конфигурации

## 6. Выводы и рекомендации для продакшена

### 6.1. Сильные стороны текущей реализации
1. **Автоматический failover**: Работает за 15-20 секунд без вмешательства человека
2. **Нулевая потеря данных**: Благодаря синхронной/асинхронной репликации
3. **Гибкая маршрутизация**: HAProxy корректно разделяет трафик записи и чтения
4. **Полный мониторинг**: Все компоненты имеют метрики в Grafana

### 6.2. Проблемы и их решения

| Проблема | Риск | Решение для продакшена |
|----------|------|------------------------|
| HAProxy как SPOF | Полная недоступность приложения | Keepalived + VRRP, мульти-A DNS |
| Потеря кворума etcd | Невозможность failover | 5 нод etcd, мониторинг кворума |
| Сетевые разрывы | Split-brain scenarios | Правильные таймауты, fencing |
| Большие транзакции при failover | Длительное восстановление | Мониторинг размера транзакций, ограничение `max_wal_size` |

### 6.3. Рекомендации по конфигурации

#### Для production etcd кластера:
```yaml
# Всегда нечетное количество для кворума
etcd_nodes: 5
# Отдельные диски для etcd данных
etcd_data_dir: /var/lib/etcd-fast-ssd
# SSL для безопасности
client_cert_auth: true
```

#### Для Patroni:
```yaml
postgresql:
  parameters:
    max_connections: 500
    shared_buffers: 4GB
    synchronous_commit: 'remote_apply'  # Для большей гарантии данных
    
# Более агрессивные health checks
retry_timeout: 15
ttl: 30

# Fencing для предотвращения split-brain
fencing: 
  loop_wait: 10
  dcstimeout: 30
```

#### Для HAProxy:
```
# Health check с меньшим интервалом
option httpchk GET /master
http-check expect status 200
default-server inter 1s fall 2 rise 2

# Резервный HAProxy с keepalived
vrrp_instance VI_1 {
    state MASTER
    virtual_router_id 51
    priority 150
    virtual_ipaddress {
        192.168.1.100
    }
}
```

### 6.4. Итог
Развернутый кластер PostgreSQL с Patroni демонстрирует высокую степень отказоустойчивости и автоматизации. Основные learnings:

1. **DCS (etcd) критичен** для работы кластера, требует отдельного внимания к надежности
2. **Failover происходит автоматически**, но клиенты должны быть готовы к временным разрывам соединения
3. **Балансировщик нагрузки** должен быть отказоустойчивым, иначе становится SPOF
4. **Мониторинг всех уровней** обязателен для оперативного реагирования на инциденты
